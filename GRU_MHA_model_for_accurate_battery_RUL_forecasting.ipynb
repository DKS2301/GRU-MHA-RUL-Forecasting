{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "authorship_tag": "ABX9TyOH6JMuFfbzUgAeTHXjkZ/Y",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/DKS2301/GRU-MHA-RUL-Forecasting/blob/main/GRU_MHA_model_for_accurate_battery_RUL_forecasting.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Battery RUL Prediction using GRU-MHA Model**"
      ],
      "metadata": {
        "id": "WeiY6GqCNjwI"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **Import Required Libraries**"
      ],
      "metadata": {
        "id": "iBFxsP1DRXf6"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.linear_model import Ridge, LinearRegression\n",
        "from sklearn.ensemble import RandomForestRegressor\n",
        "from sklearn.svm import SVR\n",
        "from sklearn.neighbors import KNeighborsRegressor\n",
        "from sklearn.metrics import mean_absolute_error, mean_squared_error, r2_score\n",
        "import tensorflow as tf\n",
        "from tensorflow import keras\n",
        "from tensorflow.keras import layers, Model\n",
        "from tensorflow.keras.callbacks import EarlyStopping, ReduceLROnPlateau\n",
        "import warnings\n",
        "warnings.filterwarnings('ignore')\n",
        "\n",
        "# Set random seeds for reproducibility\n",
        "np.random.seed(42)\n",
        "tf.random.set_seed(42)\n",
        "\n",
        "print(\"TensorFlow version:\", tf.__version__)\n",
        "print(\"GPU Available:\", tf.config.list_physical_devices('GPU'))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Hv4Eqq05NkJz",
        "outputId": "778043d9-db9e-4ce2-943e-e650f67dc1e5"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "TensorFlow version: 2.19.0\n",
            "GPU Available: [PhysicalDevice(name='/physical_device:GPU:0', device_type='GPU')]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **Data Loading Functions**"
      ],
      "metadata": {
        "id": "C5Rjp1irRc8z"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7q_qeSEuLagV"
      },
      "outputs": [],
      "source": [
        "def load_nmc_lco_data(filepath):\n",
        "    \"\"\"\n",
        "    Load NMC-LCO 18650 battery dataset\n",
        "    Expected columns: Cycle-Index, Discharge Time (s), Decrease 3.6-3.4V (s),\n",
        "    Max Discharge Voltage (V), Min Charging Voltage (V), Time at 4.15V (s),\n",
        "    Time at Constant Current (s), Charging Time (s), RUL\n",
        "    \"\"\"\n",
        "    try:\n",
        "        df = pd.read_csv(filepath)\n",
        "        print(f\"Loaded NMC-LCO data: {df.shape}\")\n",
        "        print(f\"Columns: {df.columns.tolist()}\")\n",
        "        return df\n",
        "    except Exception as e:\n",
        "        print(f\"Error loading data: {e}\")\n",
        "        return None\n",
        "\n",
        "def load_nasa_data(filepath):\n",
        "    \"\"\"\n",
        "    Load NASA battery dataset\n",
        "    Expected columns: Cycle, Time Measured(Sec), Voltage Measured(V),\n",
        "    Current Measured, Temperature Measured, Capacity(Ah), Signal Energy,\n",
        "    Fluctuation Index, Skewness Index, Kurtosis Index\n",
        "    \"\"\"\n",
        "    try:\n",
        "        df = pd.read_csv(filepath)\n",
        "        print(f\"Loaded NASA data: {df.shape}\")\n",
        "        print(f\"Columns: {df.columns.tolist()}\")\n",
        "\n",
        "        # Calculate SoH if Capacity column exists\n",
        "        if 'Capacity(Ah)' in df.columns:\n",
        "            initial_capacity = df['Capacity(Ah)'].max()\n",
        "            df['SoH'] = df['Capacity(Ah)'] / initial_capacity\n",
        "\n",
        "        return df\n",
        "    except Exception as e:\n",
        "        print(f\"Error loading data: {e}\")\n",
        "        return None\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **Feature Selection using Pearson Correlation**"
      ],
      "metadata": {
        "id": "c4__4TqsOL89"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def select_features_by_correlation(X, y, threshold=0.4):\n",
        "    \"\"\"\n",
        "    Separate features into correlated and uncorrelated based on Pearson correlation\n",
        "    \"\"\"\n",
        "    correlations = {}\n",
        "    for col in X.columns:\n",
        "        corr = np.corrcoef(X[col], y)[0, 1]\n",
        "        correlations[col] = abs(corr)\n",
        "\n",
        "    correlated_features = [k for k, v in correlations.items() if v >= threshold]\n",
        "    uncorrelated_features = [k for k, v in correlations.items() if v < threshold]\n",
        "\n",
        "    print(f\"\\nCorrelated features (>={threshold}): {correlated_features}\")\n",
        "    print(f\"Uncorrelated features (<{threshold}): {uncorrelated_features}\")\n",
        "    print(f\"\\nCorrelation values:\")\n",
        "    for k, v in sorted(correlations.items(), key=lambda x: x[1], reverse=True):\n",
        "        print(f\"  {k}: {v:.4f}\")\n",
        "\n",
        "    return correlated_features, uncorrelated_features, correlations"
      ],
      "metadata": {
        "id": "cLs1PSMKNvQs"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **Visualization Functions**"
      ],
      "metadata": {
        "id": "RiU9tLOBOnI8"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def plot_correlation_matrix(df, target_col):\n",
        "    \"\"\"Plot correlation matrix\"\"\"\n",
        "    plt.figure(figsize=(12, 10))\n",
        "    corr = df.corr()\n",
        "    sns.heatmap(corr, annot=True, fmt='.2f', cmap='coolwarm', center=0)\n",
        "    plt.title('Feature Correlation Matrix')\n",
        "    plt.tight_layout()\n",
        "    plt.show()\n",
        "\n",
        "def plot_rul_trend(df, cycle_col, target_col):\n",
        "    \"\"\"Plot RUL/SoH trend over cycles\"\"\"\n",
        "    plt.figure(figsize=(12, 6))\n",
        "    plt.plot(df[cycle_col], df[target_col], linewidth=2)\n",
        "    plt.xlabel('Cycle Number')\n",
        "    plt.ylabel(target_col)\n",
        "    plt.title(f'{target_col} vs Cycle Number')\n",
        "    plt.grid(True, alpha=0.3)\n",
        "    plt.tight_layout()\n",
        "    plt.show()\n"
      ],
      "metadata": {
        "id": "gdVSG_58Okvt"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **Dynamic Swish Activation**"
      ],
      "metadata": {
        "id": "mm8rfE3ROxvo"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "class DynamicSwish(layers.Layer):\n",
        "    \"\"\"\n",
        "    Custom Dynamic Swish activation function\n",
        "    Formula: x / (s * (1 + e^(-x)))\n",
        "    \"\"\"\n",
        "    def __init__(self, scale=1.0, **kwargs):\n",
        "        super(DynamicSwish, self).__init__(**kwargs)\n",
        "        self.scale = scale\n",
        "\n",
        "    def call(self, x):\n",
        "        return x / (self.scale * (1 + tf.exp(-x)))\n",
        "\n",
        "    def get_config(self):\n",
        "        config = super().get_config()\n",
        "        config.update({\"scale\": self.scale})\n",
        "        return config\n"
      ],
      "metadata": {
        "id": "2435xT4ZOwA2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **GRU-MHA Model Architecture**"
      ],
      "metadata": {
        "id": "OZTM5x2EO6O8"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def build_gru_mha_model(correlated_dim, uncorrelated_dim,\n",
        "                        gru_units=4, dense_units=30,\n",
        "                        mha_heads=8, mha_units=20,\n",
        "                        scale_factor=1.0):\n",
        "    \"\"\"\n",
        "    Build the hybrid GRU-MHA model with two input paths\n",
        "    \"\"\"\n",
        "    # Input layers for correlated and uncorrelated features\n",
        "    correlated_input = layers.Input(shape=(None, correlated_dim), name='correlated_input')\n",
        "    uncorrelated_input = layers.Input(shape=(None, uncorrelated_dim), name='uncorrelated_input')\n",
        "\n",
        "    # Path 1: Correlated features\n",
        "    gru1 = layers.GRU(gru_units, return_sequences=True, name='gru_corr')(correlated_input)\n",
        "    attention1 = layers.Attention(name='attention_corr')([gru1, gru1])\n",
        "    mha1 = layers.MultiHeadAttention(num_heads=mha_heads, key_dim=mha_units,\n",
        "                                     name='mha_corr')(attention1, attention1)\n",
        "    flatten1 = layers.Flatten(name='flatten_corr')(mha1)\n",
        "\n",
        "    # Path 2: Uncorrelated features\n",
        "    gru2 = layers.GRU(gru_units, return_sequences=True, name='gru_uncorr')(uncorrelated_input)\n",
        "    attention2 = layers.Attention(name='attention_uncorr')([gru2, gru2])\n",
        "    mha2 = layers.MultiHeadAttention(num_heads=mha_heads, key_dim=mha_units,\n",
        "                                     name='mha_uncorr')(attention2, attention2)\n",
        "    flatten2 = layers.Flatten(name='flatten_uncorr')(mha2)\n",
        "\n",
        "    # Concatenate both paths\n",
        "    concatenated = layers.Concatenate(name='concat')([flatten1, flatten2])\n",
        "\n",
        "    # Dense layers\n",
        "    dense1 = layers.Dense(dense_units, activation='swish', name='dense1')(concatenated)\n",
        "    dense2 = layers.Dense(dense_units//2, activation='swish', name='dense2')(dense1)\n",
        "\n",
        "    # Output with dynamic swish\n",
        "    output = layers.Dense(1, name='output_dense')(dense2)\n",
        "    output = DynamicSwish(scale=scale_factor, name='dynamic_swish')(output)\n",
        "\n",
        "    model = Model(inputs=[correlated_input, uncorrelated_input],\n",
        "                  outputs=output, name='GRU_MHA_Model')\n",
        "\n",
        "    return model\n"
      ],
      "metadata": {
        "id": "kHgz-06qO4uV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **Baseline CNN-LSTM Model**"
      ],
      "metadata": {
        "id": "N_qEegZ0PDDP"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def build_cnn_lstm_baseline(input_dim, sequence_length=1):\n",
        "    \"\"\"\n",
        "    Build baseline CNN-LSTM model for comparison\n",
        "    \"\"\"\n",
        "    inputs = layers.Input(shape=(sequence_length, input_dim))\n",
        "\n",
        "    # Conv1D layers\n",
        "    conv1 = layers.Conv1D(32, kernel_size=3, padding='same', activation='relu')(inputs)\n",
        "    conv2 = layers.Conv1D(32, kernel_size=3, padding='same', activation='relu')(conv1)\n",
        "\n",
        "    # LSTM layer\n",
        "    lstm = layers.LSTM(27, return_sequences=False)(conv2)\n",
        "\n",
        "    # Dense layers\n",
        "    dense1 = layers.Dense(30, activation='relu')(lstm)\n",
        "    output = layers.Dense(1)(dense1)\n",
        "\n",
        "    model = Model(inputs=inputs, outputs=output, name='CNN_LSTM_Baseline')\n",
        "    return model"
      ],
      "metadata": {
        "id": "nycVem99PAX5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Data Preparation**\n"
      ],
      "metadata": {
        "id": "Phij5lqTPI55"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def prepare_data(df, target_col, correlated_features, uncorrelated_features,\n",
        "                 sequence_length=1, test_size=0.2):\n",
        "    \"\"\"\n",
        "    Prepare data for training with two separate paths\n",
        "    \"\"\"\n",
        "    # Ensure features exist in dataframe\n",
        "    correlated_features = [f for f in correlated_features if f in df.columns]\n",
        "    uncorrelated_features = [f for f in uncorrelated_features if f in df.columns]\n",
        "\n",
        "    if len(correlated_features) == 0:\n",
        "        correlated_features = uncorrelated_features[:1]\n",
        "        uncorrelated_features = uncorrelated_features[1:]\n",
        "\n",
        "    X_corr = df[correlated_features].values\n",
        "    X_uncorr = df[uncorrelated_features].values\n",
        "    y = df[target_col].values\n",
        "\n",
        "    # Normalize features\n",
        "    scaler_corr = StandardScaler()\n",
        "    scaler_uncorr = StandardScaler()\n",
        "    scaler_y = StandardScaler()\n",
        "\n",
        "    X_corr_scaled = scaler_corr.fit_transform(X_corr)\n",
        "    X_uncorr_scaled = scaler_uncorr.fit_transform(X_uncorr)\n",
        "    y_scaled = scaler_y.fit_transform(y.reshape(-1, 1)).flatten()\n",
        "\n",
        "    # Reshape for sequence input\n",
        "    X_corr_seq = X_corr_scaled.reshape(-1, sequence_length, len(correlated_features))\n",
        "    X_uncorr_seq = X_uncorr_scaled.reshape(-1, sequence_length, len(uncorrelated_features))\n",
        "\n",
        "    # Split data\n",
        "    indices = np.arange(len(y_scaled))\n",
        "    train_idx, test_idx = train_test_split(indices, test_size=test_size, random_state=42)\n",
        "\n",
        "    X_corr_train = X_corr_seq[train_idx]\n",
        "    X_corr_test = X_corr_seq[test_idx]\n",
        "    X_uncorr_train = X_uncorr_seq[train_idx]\n",
        "    X_uncorr_test = X_uncorr_seq[test_idx]\n",
        "    y_train = y_scaled[train_idx]\n",
        "    y_test = y_scaled[test_idx]\n",
        "\n",
        "    return {\n",
        "        'X_corr_train': X_corr_train, 'X_corr_test': X_corr_test,\n",
        "        'X_uncorr_train': X_uncorr_train, 'X_uncorr_test': X_uncorr_test,\n",
        "        'y_train': y_train, 'y_test': y_test,\n",
        "        'scaler_corr': scaler_corr, 'scaler_uncorr': scaler_uncorr,\n",
        "        'scaler_y': scaler_y, 'train_idx': train_idx, 'test_idx': test_idx\n",
        "    }\n"
      ],
      "metadata": {
        "id": "aKxfb8-WPG6z"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "# **Training Function**"
      ],
      "metadata": {
        "id": "imOmOwNUPU85"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def train_model(model, data_dict, epochs=250, batch_size=32, patience=50):\n",
        "    \"\"\"\n",
        "    Train the GRU-MHA model\n",
        "    \"\"\"\n",
        "    # Callbacks\n",
        "    early_stopping = EarlyStopping(\n",
        "        monitor='val_loss',\n",
        "        patience=patience,\n",
        "        restore_best_weights=True,\n",
        "        verbose=1\n",
        "    )\n",
        "\n",
        "    reduce_lr = ReduceLROnPlateau(\n",
        "        monitor='val_loss',\n",
        "        factor=0.3,\n",
        "        patience=patience,\n",
        "        min_lr=1e-7,\n",
        "        verbose=1\n",
        "    )\n",
        "\n",
        "    # Compile model\n",
        "    optimizer = keras.optimizers.Nadam(learning_rate=0.001)\n",
        "    model.compile(optimizer=optimizer, loss='mse', metrics=['mae'])\n",
        "\n",
        "    # Train\n",
        "    history = model.fit(\n",
        "        [data_dict['X_corr_train'], data_dict['X_uncorr_train']],\n",
        "        data_dict['y_train'],\n",
        "        validation_split=0.2,\n",
        "        epochs=epochs,\n",
        "        batch_size=batch_size,\n",
        "        callbacks=[early_stopping, reduce_lr],\n",
        "        verbose=1\n",
        "    )\n",
        "\n",
        "    return history\n"
      ],
      "metadata": {
        "id": "EKPsz7HnPScT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **Ridge Regression Final Stage**"
      ],
      "metadata": {
        "id": "2iU94t6-PewR"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def apply_ridge_regression(model, data_dict, alpha=1.5):\n",
        "    \"\"\"\n",
        "    Apply ridge regression as the final forecasting stage\n",
        "    \"\"\"\n",
        "    # Get predictions from the model\n",
        "    train_pred = model.predict([data_dict['X_corr_train'], data_dict['X_uncorr_train']])\n",
        "    test_pred = model.predict([data_dict['X_corr_test'], data_dict['X_uncorr_test']])\n",
        "\n",
        "    # Train ridge regression on model outputs\n",
        "    ridge = Ridge(alpha=alpha, solver='cholesky')\n",
        "    ridge.fit(train_pred, data_dict['y_train'])\n",
        "\n",
        "    # Final predictions\n",
        "    final_train_pred = ridge.predict(train_pred)\n",
        "    final_test_pred = ridge.predict(test_pred)\n",
        "\n",
        "    return final_train_pred, final_test_pred, ridge"
      ],
      "metadata": {
        "id": "tx_tUREnPeCa"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **Evaluation Metrics**"
      ],
      "metadata": {
        "id": "gQHT91yePnIa"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def evaluate_model(y_true, y_pred, scaler_y, model_name=\"Model\"):\n",
        "    \"\"\"\n",
        "    Calculate and display evaluation metrics\n",
        "    \"\"\"\n",
        "    # Inverse transform\n",
        "    y_true_orig = scaler_y.inverse_transform(y_true.reshape(-1, 1)).flatten()\n",
        "    y_pred_orig = scaler_y.inverse_transform(y_pred.reshape(-1, 1)).flatten()\n",
        "\n",
        "    mae = mean_absolute_error(y_true_orig, y_pred_orig)\n",
        "    mse = mean_squared_error(y_true_orig, y_pred_orig)\n",
        "    rmse = np.sqrt(mse)\n",
        "    r2 = r2_score(y_true_orig, y_pred_orig)\n",
        "\n",
        "    print(f\"\\n{model_name} Performance:\")\n",
        "    print(f\"  MAE: {mae:.6f}\")\n",
        "    print(f\"  MSE: {mse:.6f}\")\n",
        "    print(f\"  RMSE: {rmse:.6f}\")\n",
        "    print(f\"  R² Score: {r2:.6f} ({r2*100:.2f}%)\")\n",
        "\n",
        "    return {'mae': mae, 'mse': mse, 'rmse': rmse, 'r2': r2}\n"
      ],
      "metadata": {
        "id": "f7Jdi6VLPlnF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **Plotting Functions:**"
      ],
      "metadata": {
        "id": "ChNMvumVPwgY"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def plot_training_history(history):\n",
        "    \"\"\"Plot training and validation loss\"\"\"\n",
        "    fig, axes = plt.subplots(1, 2, figsize=(15, 5))\n",
        "\n",
        "    # Loss\n",
        "    axes[0].plot(history.history['loss'], label='Train Loss')\n",
        "    axes[0].plot(history.history['val_loss'], label='Val Loss')\n",
        "    axes[0].set_xlabel('Epoch')\n",
        "    axes[0].set_ylabel('Loss')\n",
        "    axes[0].set_title('Training and Validation Loss')\n",
        "    axes[0].legend()\n",
        "    axes[0].grid(True, alpha=0.3)\n",
        "\n",
        "    # MAE\n",
        "    axes[1].plot(history.history['mae'], label='Train MAE')\n",
        "    axes[1].plot(history.history['val_mae'], label='Val MAE')\n",
        "    axes[1].set_xlabel('Epoch')\n",
        "    axes[1].set_ylabel('MAE')\n",
        "    axes[1].set_title('Training and Validation MAE')\n",
        "    axes[1].legend()\n",
        "    axes[1].grid(True, alpha=0.3)\n",
        "\n",
        "    plt.tight_layout()\n",
        "    plt.show()\n",
        "\n",
        "def plot_predictions(y_true, y_pred, scaler_y, title=\"Predictions vs Actual\"):\n",
        "    \"\"\"Plot predicted vs actual values\"\"\"\n",
        "    y_true_orig = scaler_y.inverse_transform(y_true.reshape(-1, 1)).flatten()\n",
        "    y_pred_orig = scaler_y.inverse_transform(y_pred.reshape(-1, 1)).flatten()\n",
        "\n",
        "    plt.figure(figsize=(12, 6))\n",
        "    plt.plot(y_true_orig, label='Actual', linewidth=2, alpha=0.7)\n",
        "    plt.plot(y_pred_orig, label='Predicted', linewidth=2, alpha=0.7)\n",
        "    plt.xlabel('Sample Index')\n",
        "    plt.ylabel('Value')\n",
        "    plt.title(title)\n",
        "    plt.legend()\n",
        "    plt.grid(True, alpha=0.3)\n",
        "    plt.tight_layout()\n",
        "    plt.show()\n",
        "\n",
        "    # Scatter plot\n",
        "    plt.figure(figsize=(8, 8))\n",
        "    plt.scatter(y_true_orig, y_pred_orig, alpha=0.5)\n",
        "    plt.plot([y_true_orig.min(), y_true_orig.max()],\n",
        "             [y_true_orig.min(), y_true_orig.max()],\n",
        "             'r--', linewidth=2)\n",
        "    plt.xlabel('Actual Values')\n",
        "    plt.ylabel('Predicted Values')\n",
        "    plt.title('Predicted vs Actual Values')\n",
        "    plt.grid(True, alpha=0.3)\n",
        "    plt.tight_layout()\n",
        "    plt.show()"
      ],
      "metadata": {
        "id": "Vg_wLgTaPuLH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **Comparison with ML Models**"
      ],
      "metadata": {
        "id": "aH5uTYxIP4od"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def compare_ml_models(X_train, X_test, y_train, y_test, scaler_y):\n",
        "    \"\"\"\n",
        "    Compare with traditional ML models\n",
        "    \"\"\"\n",
        "    models = {\n",
        "        'KNN': KNeighborsRegressor(n_neighbors=5),\n",
        "        'SVR': SVR(kernel='rbf'),\n",
        "        'Random Forest': RandomForestRegressor(n_estimators=150, max_depth=10,\n",
        "                                               min_samples_leaf=4, random_state=42),\n",
        "        'Linear Regression': LinearRegression()\n",
        "    }\n",
        "\n",
        "    results = {}\n",
        "\n",
        "    for name, model in models.items():\n",
        "        print(f\"\\nTraining {name}...\")\n",
        "        model.fit(X_train, y_train)\n",
        "        y_pred = model.predict(X_test)\n",
        "        metrics = evaluate_model(y_test, y_pred, scaler_y, name)\n",
        "        results[name] = metrics\n",
        "\n",
        "    return results"
      ],
      "metadata": {
        "id": "j_EF9sb2P2Xv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **Forward Feature Selection**"
      ],
      "metadata": {
        "id": "VuS6ix1mQASz"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def forward_feature_selection(df, target_col, features):\n",
        "    \"\"\"\n",
        "    Evaluate feature importance using forward selection\n",
        "    \"\"\"\n",
        "    selected_features = []\n",
        "    remaining_features = features.copy()\n",
        "    r2_scores = []\n",
        "\n",
        "    print(\"\\nForward Feature Selection:\")\n",
        "\n",
        "    for i in range(len(features)):\n",
        "        best_r2 = -np.inf\n",
        "        best_feature = None\n",
        "\n",
        "        for feature in remaining_features:\n",
        "            test_features = selected_features + [feature]\n",
        "            X = df[test_features].values\n",
        "            y = df[target_col].values\n",
        "\n",
        "            # Simple train-test split\n",
        "            X_train, X_test, y_train, y_test = train_test_split(\n",
        "                X, y, test_size=0.2, random_state=42\n",
        "            )\n",
        "\n",
        "            # Train simple linear regression\n",
        "            model = LinearRegression()\n",
        "            model.fit(X_train, y_train)\n",
        "            y_pred = model.predict(X_test)\n",
        "            r2 = r2_score(y_test, y_pred)\n",
        "\n",
        "            if r2 > best_r2:\n",
        "                best_r2 = r2\n",
        "                best_feature = feature\n",
        "\n",
        "        selected_features.append(best_feature)\n",
        "        remaining_features.remove(best_feature)\n",
        "        r2_scores.append(best_r2)\n",
        "\n",
        "        print(f\"  Step {i+1}: Added '{best_feature}' (R² = {best_r2:.4f})\")\n",
        "\n",
        "    # Plot results\n",
        "    plt.figure(figsize=(12, 6))\n",
        "    plt.plot(range(1, len(features)+1), r2_scores, marker='o', linewidth=2)\n",
        "    plt.xlabel('Number of Features')\n",
        "    plt.ylabel('R² Score')\n",
        "    plt.title('Forward Feature Selection Results')\n",
        "    plt.grid(True, alpha=0.3)\n",
        "    plt.xticks(range(1, len(features)+1))\n",
        "    for i, feature in enumerate(selected_features):\n",
        "        plt.text(i+1, r2_scores[i], feature, ha='center', va='bottom', fontsize=8)\n",
        "    plt.tight_layout()\n",
        "    plt.show()\n",
        "\n",
        "    return selected_features, r2_scores\n"
      ],
      "metadata": {
        "id": "ITzmwysNQA3D"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **Main Execution Pipeline**"
      ],
      "metadata": {
        "id": "-GX8SB3PQOZg"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def run_complete_pipeline(data_path, dataset_type='NMC', target_col='RUL',\n",
        "                          correlation_threshold=0.4, scale_factor=1.0):\n",
        "    \"\"\"\n",
        "    Complete pipeline for battery RUL/SoH prediction\n",
        "\n",
        "    Parameters:\n",
        "    - data_path: Path to the CSV file\n",
        "    - dataset_type: 'NMC' or 'NASA'\n",
        "    - target_col: Target column name ('RUL' or 'SoH')\n",
        "    - correlation_threshold: Threshold for feature correlation\n",
        "    - scale_factor: Scale factor for dynamic swish activation\n",
        "    \"\"\"\n",
        "\n",
        "    print(\"=\"*80)\n",
        "    print(\"BATTERY RUL/SOH PREDICTION PIPELINE\")\n",
        "    print(\"=\"*80)\n",
        "\n",
        "    # Load data\n",
        "    if dataset_type == 'NMC':\n",
        "        df = load_nmc_lco_data(data_path)\n",
        "    else:\n",
        "        df = load_nasa_data(data_path)\n",
        "\n",
        "    if df is None:\n",
        "        return None\n",
        "\n",
        "    # Exploratory analysis\n",
        "    print(\"\\n\" + \"=\"*80)\n",
        "    print(\"DATA EXPLORATION\")\n",
        "    print(\"=\"*80)\n",
        "    print(df.describe())\n",
        "\n",
        "    # Plot correlation matrix\n",
        "    plot_correlation_matrix(df, target_col)\n",
        "\n",
        "    # Plot RUL/SoH trend\n",
        "    cycle_col = 'Cycle-Index' if 'Cycle-Index' in df.columns else 'Cycle'\n",
        "    plot_rul_trend(df, cycle_col, target_col)\n",
        "\n",
        "    # Feature selection\n",
        "    print(\"\\n\" + \"=\"*80)\n",
        "    print(\"FEATURE SELECTION\")\n",
        "    print(\"=\"*80)\n",
        "\n",
        "    feature_cols = [col for col in df.columns if col != target_col]\n",
        "    X = df[feature_cols]\n",
        "    y = df[target_col]\n",
        "\n",
        "    correlated_features, uncorrelated_features, correlations = \\\n",
        "        select_features_by_correlation(X, y, correlation_threshold)\n",
        "\n",
        "    # Prepare data\n",
        "    print(\"\\n\" + \"=\"*80)\n",
        "    print(\"DATA PREPARATION\")\n",
        "    print(\"=\"*80)\n",
        "\n",
        "    data_dict = prepare_data(df, target_col, correlated_features,\n",
        "                            uncorrelated_features, sequence_length=1)\n",
        "\n",
        "    print(f\"Training samples: {len(data_dict['y_train'])}\")\n",
        "    print(f\"Test samples: {len(data_dict['y_test'])}\")\n",
        "    print(f\"Correlated features shape: {data_dict['X_corr_train'].shape}\")\n",
        "    print(f\"Uncorrelated features shape: {data_dict['X_uncorr_train'].shape}\")\n",
        "\n",
        "    # Build model\n",
        "    print(\"\\n\" + \"=\"*80)\n",
        "    print(\"MODEL BUILDING\")\n",
        "    print(\"=\"*80)\n",
        "\n",
        "    model = build_gru_mha_model(\n",
        "        correlated_dim=len(correlated_features),\n",
        "        uncorrelated_dim=len(uncorrelated_features),\n",
        "        gru_units=4,\n",
        "        dense_units=30,\n",
        "        mha_heads=8,\n",
        "        mha_units=20,\n",
        "        scale_factor=scale_factor\n",
        "    )\n",
        "\n",
        "    model.summary()\n",
        "\n",
        "    # Train model\n",
        "    print(\"\\n\" + \"=\"*80)\n",
        "    print(\"MODEL TRAINING\")\n",
        "    print(\"=\"*80)\n",
        "\n",
        "    history = train_model(model, data_dict, epochs=250, batch_size=32, patience=50)\n",
        "\n",
        "    # Plot training history\n",
        "    plot_training_history(history)\n",
        "\n",
        "    # Apply ridge regression\n",
        "    print(\"\\n\" + \"=\"*80)\n",
        "    print(\"APPLYING RIDGE REGRESSION\")\n",
        "    print(\"=\"*80)\n",
        "\n",
        "    final_train_pred, final_test_pred, ridge_model = \\\n",
        "        apply_ridge_regression(model, data_dict, alpha=1.5)\n",
        "\n",
        "    # Evaluate\n",
        "    print(\"\\n\" + \"=\"*80)\n",
        "    print(\"MODEL EVALUATION\")\n",
        "    print(\"=\"*80)\n",
        "\n",
        "    train_metrics = evaluate_model(data_dict['y_train'], final_train_pred,\n",
        "                                   data_dict['scaler_y'], \"Training Set\")\n",
        "    test_metrics = evaluate_model(data_dict['y_test'], final_test_pred,\n",
        "                                  data_dict['scaler_y'], \"Test Set\")\n",
        "\n",
        "    # Plot predictions\n",
        "    plot_predictions(data_dict['y_test'], final_test_pred,\n",
        "                    data_dict['scaler_y'],\n",
        "                    f\"Test Set: {target_col} Predictions vs Actual\")\n",
        "\n",
        "    # Compare with ML models (flatten features for comparison)\n",
        "    print(\"\\n\" + \"=\"*80)\n",
        "    print(\"COMPARISON WITH ML MODELS\")\n",
        "    print(\"=\"*80)\n",
        "\n",
        "    X_train_flat = np.concatenate([\n",
        "        data_dict['X_corr_train'].reshape(len(data_dict['X_corr_train']), -1),\n",
        "        data_dict['X_uncorr_train'].reshape(len(data_dict['X_uncorr_train']), -1)\n",
        "    ], axis=1)\n",
        "\n",
        "    X_test_flat = np.concatenate([\n",
        "        data_dict['X_corr_test'].reshape(len(data_dict['X_corr_test']), -1),\n",
        "        data_dict['X_uncorr_test'].reshape(len(data_dict['X_uncorr_test']), -1)\n",
        "    ], axis=1)\n",
        "\n",
        "    ml_results = compare_ml_models(X_train_flat, X_test_flat,\n",
        "                                   data_dict['y_train'], data_dict['y_test'],\n",
        "                                   data_dict['scaler_y'])\n",
        "\n",
        "    # Feature importance\n",
        "    print(\"\\n\" + \"=\"*80)\n",
        "    print(\"FEATURE IMPORTANCE ANALYSIS\")\n",
        "    print(\"=\"*80)\n",
        "\n",
        "    all_features = correlated_features + uncorrelated_features\n",
        "    selected_features, r2_scores = forward_feature_selection(df, target_col, all_features)\n",
        "\n",
        "    print(\"\\n\" + \"=\"*80)\n",
        "    print(\"PIPELINE COMPLETED SUCCESSFULLY\")\n",
        "    print(\"=\"*80)\n",
        "\n",
        "    return {\n",
        "        'model': model,\n",
        "        'ridge': ridge_model,\n",
        "        'data_dict': data_dict,\n",
        "        'history': history,\n",
        "        'test_metrics': test_metrics,\n",
        "        'ml_results': ml_results,\n",
        "        'correlations': correlations,\n",
        "        'selected_features': selected_features\n",
        "    }"
      ],
      "metadata": {
        "id": "xzi2QR5gQMQN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **Example Usage**"
      ],
      "metadata": {
        "id": "GOOE41q8QUWo"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "# For NMC-LCO 18650 Battery Dataset:\n",
        "results = run_complete_pipeline(\n",
        "    data_path='path/to/nmc_lco_battery_data.csv',\n",
        "    dataset_type='NMC',\n",
        "    target_col='RUL',\n",
        "    correlation_threshold=0.4,\n",
        "    scale_factor=1.0\n",
        ")\n",
        "\n",
        "# For NASA Battery Dataset:\n",
        "results = run_complete_pipeline(\n",
        "    data_path='path/to/nasa_battery_B0005.csv',\n",
        "    dataset_type='NASA',\n",
        "    target_col='SoH',\n",
        "    correlation_threshold=0.4,\n",
        "    scale_factor=8.0  # Use 8.0 for values between 0-2\n",
        ")\n",
        "\n",
        "# Access results:\n",
        "if results:\n",
        "    print(f\"Test R² Score: {results['test_metrics']['r2']:.4f}\")\n",
        "    print(f\"Test MAE: {results['test_metrics']['mae']:.6f}\")\n",
        "\n",
        "    # Save model\n",
        "    results['model'].save('gru_mha_model.h5')\n",
        "\n",
        "    # Access predictions, etc.\n",
        "\n",
        "print(\"\\n\" + \"=\"*80)\n",
        "print(\"IMPLEMENTATION READY\")\n",
        "print(\"=\"*80)\n",
        "print(\"\\nTo use this implementation:\")\n",
        "print(\"1. Load your battery dataset (NMC-LCO or NASA)\")\n",
        "print(\"2. Run: results = run_complete_pipeline('your_data.csv', dataset_type='NMC' or 'NASA')\")\n",
        "print(\"3. The pipeline will automatically handle everything!\")\n",
        "print(\"\\nMake sure your CSV has the appropriate columns as described in the paper.\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kLyQQbCxOVZN",
        "outputId": "782b6bcb-0a6d-4615-8c2e-a06c228d02cb"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "================================================================================\n",
            "BATTERY RUL/SOH PREDICTION PIPELINE\n",
            "================================================================================\n",
            "Error loading data: [Errno 2] No such file or directory: 'path/to/nmc_lco_battery_data.csv'\n",
            "================================================================================\n",
            "BATTERY RUL/SOH PREDICTION PIPELINE\n",
            "================================================================================\n",
            "Error loading data: [Errno 2] No such file or directory: 'path/to/nasa_battery_B0005.csv'\n",
            "\n",
            "================================================================================\n",
            "IMPLEMENTATION READY\n",
            "================================================================================\n",
            "\n",
            "To use this implementation:\n",
            "1. Load your battery dataset (NMC-LCO or NASA)\n",
            "2. Run: results = run_complete_pipeline('your_data.csv', dataset_type='NMC' or 'NASA')\n",
            "3. The pipeline will automatically handle everything!\n",
            "\n",
            "Make sure your CSV has the appropriate columns as described in the paper.\n"
          ]
        }
      ]
    }
  ]
}